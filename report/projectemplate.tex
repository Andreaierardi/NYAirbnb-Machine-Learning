\documentclass{FR16} 
\usepackage{listings}
\usepackage{pdfpages}

\begin{document}

\maketitle

% ML template reports
% https://www.cs.utexas.edu/~mooney/cs391L/paper-template.html
%https://github.com/udacity/machine-learning/blob/master/projects/capstone/report-example-1.pdf
\newpage
\tableofcontents
\newpage
\section{Abstract}
The aim of the project is to analyse, develop prediction models and define data clusters from the "New York City Airbnb Open Data" from a Kaggle competition.
\\ In particular, one part is focused on the development of predictive models to forecast house prices using these Supervised Learning technics:\begin{itemize}
\itemsep0em 
\item Linear Regression
\item Decision Tree
\item Random Forest
\item Ranger Random Forest
\item Neural Newtworks
\end{itemize}
For each of these, a comparison between the Mean Square Error and between all $R^2$ measure of all methods has been made to highlight which have the best performance. Also, for training all the models, a partition of the dataset in three parts has been applied: entire dataset, filtered by neighboorhood group and filtered for neighboorhood group and room type. In this way, it is possible to check the perfomance giving less or more features in input.\\\\
The second part is focused on the cluster and data reduction technics using these Unsupervised Learning technics:
\begin{itemize}
\itemsep0em
\item K-means Algorithm 
\item Clustering for mixed-type data
\item Principal Component Analysis
\end{itemize}

%%Eachreport must contain:\\
%•shortabstract: what are your going to present %in the report\\
%•statementof the problem/goalof the analysis %and description of thedata set(s)\\
%•list of three to fivefindings/keypoints\\
%•the analysis with wisecommentary\\
%•(optional) theoretical background of the used %methods\\
%•conclusions(should include the findings/%keypoints)\\
%•theAppendix, containing all the R codeNotice:
%\\•Thepaper lengthisirrelevant provided that %the content is correct.
%\\•No R code in the main text.The R code must %be confined to the appendix\\
%•The report should be prepared inPDFonly

\newpage
\section{Problem Definition and Algorithm}
\subsection{Two main Goals}

\subsubsection{Develop predictive models for price}
First objective is the forecast of the prices given some input informations. This could be usefull for a lot of scenarios. For example from a AirBnB user point of view, he/she would like to get the houses more in line with his preference choice; or for a host point of view, where given the position and other information he/she could get information about the possible per day price of his house in New York City. 

\subsubsection{Define clusters and groups}
Second objective is the definition of group or partition between the houses with different characteristics. For a user point of view could be usefull to have information about similar houses before the booking, giving in input some information about their preference. This could be also usefull after the booking for a suggestion analysis giving the information about last booked houses in NewYork or houses in similar cities around the globe.
\\\\
\subsection{Algorithms}
\subsubsection{Linear Regression}
 Linear regression  is a linear approach to modeling the relationship between a dependent variable and one or more independent variables. Linear regression should be suitable, since there could be a linear relationship between the position and the price. In the city center there will be the expensive houses, while in the outskirts there will be the cheaper ones.
  \subsubsection{Decison Trees}
In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making.

 \subsubsection{Random Forest}
 Random forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees.
 \subsubsection{Ranger Random Forest}
Ranger is a fast implementation of random forests or recursive partitioning, particularly suited for high dimensional data. 

\subsubsection{Neural Networks}

\subsubsection{K-means}

\subsubsection{Principal Component Analysis}

\newpage
\section{Experimental Evaluation}

\subsection{Methodology}
\subsubsection{Data Inspection}
The dataset is a Kaggle competiotion dataset, called the New York City Airbnb Open Data. It contains 48.000 data points for each different column.
The dataset is structured with 16 columns: 
\begin{itemize}
\itemsep0em 
\item \textbf{id}
\item \textbf{name}: name of the listing
\item \textbf{host\_id}
\item \textbf{host\_name}
\item \textbf{neighbourhood\_group}: location
\item \textbf{neighbourhood}: area
\item \textbf{latitude}: coordinates
\item \textbf{longitude}: coordinates
\item \textbf{room\_type}: space type
\item \textbf{price}:  in dollars
\item \textbf{minimum\_nights}: amount of nights minimum
\item \textbf{number\_of\_reviews}: number of reviews
\item \textbf{last\_review}: latest review
\item \textbf{reviews\_per\_month}: number of reviews per month
\item \textbf{calculated\_host\_listings\_count}: amount of listing per host
\item  \textbf{availability\_365}: number of days when listing is available for booking\\
\end{itemize}
It has been select only 5 of this features since these are the most important for determine the prices and also the different groups of clusters. These feature selected are: latitude, longitude, room type, neighbourhood and the price itself that will be compared with the prediction.\\\\
From the Figure 1 is possible to see the distribution of the price. The minimum price is $0$ and the maximum price is $10000\$$. It is not possible to rent a house for free, so it is possible to filter the price with a price higher than $15\$$ and lower than $500\$$. It is possible that a luxury house cost a lot per day, but these value can not be consider in the model and can be consider as outliers. The reason is that the third quantile has a value of $175\$$ which is a far value from $10000\$$. It is also been checked the null and missing value and have not been found in the dataset apart from the reviews\_per\_month column. This feature has been not taken in account as possible one to the model training. 
\\
\\From Figure2, it is possible to see the distribution of all houses in New York City and the price. It can be noticed that the prices for the most part are in the range $0$-$500\$$ and only a low number of istances have a price greater than $500\$$. This picture is really  informative.
 \begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{figures/figure1.jpg} 
\caption{\label{fig:1}Price summary}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/figure2.pdf} 
\caption{\label{fig:2}Distribution of all houses in NY colored by prices}
\end{figure}

\subsubsection{Data Cleaning and Pre-processing}
The dataset has more or less 48.000 data points for each column, so an important part of this work was the pre-processing since the large amount of data. Moreover, running different methods and algorithms on the entire dataset has an high computation cost. An important choice to made is that the user is able to discriminate which type of house has a particular interest. Then, it can be assumed that the user chooses which is the price range of interest and also the type of room. For the simplicity of the project we will focus on the Manhattan region for the popularity, a range of price from $15\$$ to $500\$$ and an entire apartment type of room. The project can be extended easily to the entire dataset, based on the user preference. 
Each variable choosed in this project have been rescaled to let the model perform and learn better, apart from latitude and longitude since the rescaling should not have a real meaning.
\\ Also categorical variables have been rescaled assigning a numerical value to each category, resulting as factors. \\
The selected categorical features are: neighbourhood\_group and room type.\\
The selected numerical features are: latitude, longitude and price.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/figure3.pdf} 
\caption{\label{fig:3} Distribution of all houses in NY of price between $15\$$ and $500\$$ per day}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/figure4.PNG} 
\caption{\label{fig:4}  Approximated distribution map of all houses in Manhattan }
\end{figure}



\subsection{Results}

\subsubsection{Linear Regression Results}
For the Linear regression the model give different results based on the variables used. \\\\
\textbf{Linear Regression selecting the Neighboorhood group}\\
For semplicity, the tests have been only taken filtering for Manhattan data points, but changing the neighboorhood the results are similar. 
Results are acceptable (Figure 5), given a $R^2$ value of $0.4011$ and a Mean Square Error, comparing prediction and test set, of $0.67$.
\\\\ \textbf{Linear Regression selecting the Neighboorhood group and the type of room}\\
As in the previous case the tests are for Manhattan and for Entire home/Aparment type of room.\\
The model output (Figure 6) a value of $R^2$ equals to $ 0.05953$ which is low  and a Mean Square Error, comparing prediction and test, of $1.23$.
\\\\ \textbf{Linear Regression without filters}\\
The model (Figure 7) obtain a $R^2$ value of $0.4031$ which is acceptable and a Mean Square Error, comparing prediction and test set, of $ 0.59$.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/lm2.PNG} 
\caption{\label{fig:6}  Linear Regression output filtering by  Manhattan}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/lm3.PNG} 
\caption{\label{fig:7}  Linear Regression output filtering by  Manhattan and Entire home/Apartment }
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/lm1.PNG} 
\caption{\label{fig:5} Linear Regression output without filters }
\end{figure}





\subsubsection{Decison Trees Results}
For the Decision tree the model give different results based on the variables used.
\\\\ \textbf{Decison tree  without filters}\\
\\\\
\textbf{Decison tree selecting the Neighboorhood group}\\
\\\\ 
\textbf{Decison tree  selecting the Neighboorhood group and room type}
\subsubsection{Random Forest Results}
There were no problem running Random Forest regression for the price, givin the default parameters. For the parameter tuning there were no possibilities for the entire dataset, due to the large number of data points. For the filtered dataset, instead, were possible to tune the mtry, number of maximum nodes and number of trees.
\\\\ \textbf{Random Forest  without filters}\\
There were no possibility to run a forecast of the price
\\\\
\textbf{Random Forest selecting the Neighboorhood group}\\
\\\\ 
\textbf{Random Forest selecting the Neighboorhood group and room type}
Using the tuning of the parameter the results are slightly better. The model start with a $23\%$ explained variance to a value of $25\%$.
\subsubsection{Ranger Random Forest Results}
Ranger Random Forest is known to be computationally light with respect to the classic Random Forest. In fact, for the tuning part there were not problem in running it for the entire dataset. 
\\\\ \textbf{Ranger without filters}\\
Ranger outputs for the entire dataset are consistent. We have a $R^2$ of 0.47 and OOB error of .
\\\\
\textbf{Ranger selecting the Neighboorhood group}\\
The dataset filtered by Neighboorhood ouputs a value of 0.4 $R^2$ and a OOB error of.
\\\\ 
\textbf{Ranger selecting the Neighboorhood group and room type}
The dataset filtered by neighboorhood and room type gives as result a $R^2$ of 0.25.
\\\\
\begin{center}
\begin{tabular}{l c c c }
\arrayrulecolor{Azzurro}
\hline
{\bfseries Dataset
} & MSE & $R^2$ & OOB \\
\hline
{\bfseries Linear Regression} & .. & .. & $\backslash$ \\
{\bfseries Decision Tree} & .. & .. & $\backslash$ \\
{\bfseries Random Forest} & .. & .. & .. \\
{\bfseries Ranger RF} & .. & .. & .. \\
{\bfseries Neural Networks} & .. & .. & .. \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{l c c c }
\arrayrulecolor{Azzurro}
\hline
{\bfseries Filter: Manhattan
} & MSE & $R^2$ & OOB \\
\hline
{\bfseries Linear Regression} & .. & .. & $\backslash$ \\
{\bfseries Decision Tree} & .. & .. & $\backslash$ \\
{\bfseries Random Forest} & .. & .. & .. \\
{\bfseries Ranger RF} & .. & .. & .. \\
{\bfseries Neural Networks} & .. & .. & .. \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{l c c c }
\arrayrulecolor{Azzurro}
\hline
{\bfseries Filter: Manhattan \& Apt
} & MSE & $R^2$ & OOB \\
\hline
{\bfseries Linear Regression} & .. & .. & $\backslash$ \\
{\bfseries Decision Tree} & .. & .. & $\backslash$ \\
{\bfseries Random Forest} & .. & .. & .. \\
{\bfseries Ranger RF} & .. & .. & .. \\
{\bfseries Neural Networks} & .. & .. & .. \\
\hline
\end{tabular}
\end{center}

\subsubsection{Neural Network Results}

\subsubsection{K-means Results}


\subsubsection{Principal Component Analysis Results}

\newpage
\subsection{Discussion}

\subsubsection{Linear Regression Discussion}
Linear regression model gives interesting results for the non-filtered dataset and also for the filtered by neighbourhood group.
All variable results to rejected by Null hyphotesis, so the model depends on all the selected variables.
Latitude and longitude are correlated with the target, room type is strongly positive correlated with the price and neighbourhood group does not seem to have
a great contribution in the prediction of the price.
\\

\subsubsection{Decision Tree Discussion}
The performance with respect to the other models are not the best, but acceptable. The predicton results are not also very precised for the filtered neighboorhood 
and room type. Also, the plots of the predicted value are not so consistent since the values are divided in category which correspond to the leaves that are not so strong
with respect to the other model predictions.
\\

\subsubsection{Random Forest Discussion}
Random forest outputs consistent results and performs  better than linear regression and decision tree. 
Parameters tuning does not give big improvement in performance and also are computationally expensive.
\\

\subsubsection{Ranger Random Forest Discussion}
Results of Ranger are the best with respect to the previous models. Also the tuning part was fast and computationally cheaper than 
the classic Random Forest model but does not give great improvements in performance.
\\
\subsubsection{Neural Network Discussion}
\subsubsection{K-Means Discussion}
\subsubsection{Principal Component Analysis Discussion}

\section{Conclusion}

From the result the method with the most higher accuracy is the Random Forest method... while the worst are ....
\\Moreover, Random Forest method is also the worst in term of computation time for the tuning part since it takes for a configuration with 4 core, more or less 1 hour to tune the parameters. 

\subsection{Linear Regression}


\subsection{Decision Tree}
Decision  tree are one of the most used model in the Machine Learning world since are very familiar to human users and can be easily plotted. 
\\



\subsection{Random Forest}
Random Forest is an ensemble method which use a combination of decision tree to get the prediction.
\subsubsection{Ranger Random Forest}
Ranger Random Forest is a computationally light model which results are very close the classical Random Forest.
\\





\newpage
\section{Appendix}
%\includepdf[pages=-]{project.pdf}
%\noinden

\newpage


\subsection{Footnote}
You can create a footnote like this.\footnote{I created a footnote.}



\newpage
\begin{thebibliography}{9}
\bibitem{giusti}
Giusti, Santochi, \emph{Tecnologia Meccanica e Studi di Fabbricazione}. Casa Editrice Ambrosiana, Seconda Edizione
\bibitem{mechteacher}
Mechteacher, \emph{Knuckle Joint – Introduction, Parts and Applications},\\ http://mechteacher.com/knuckle-joint/
\bibitem{totalmateria}
Totalmateria, \emph{G32NiCrMo8}, http://www.totalmateria.com 
\bibitem{sandvik}
Sandvik Coromant,\emph{Catalogo  generale  2018},   http://www.coromant.sandvik.com/it
\bibitem{uni}
Norme UNI, Ente nazionale italiano di unificazione
\end{thebibliography}

\end{document}