---
title: "Statistical Learning Project - Unsupervised Learning"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```



```{r}
#https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data
library(ggplot2)
library(ggmap)
library(tidyr)
library(cowplot)
library(magick)
library(dplyr)
#world_map <- map_data("newyork")


```


# Read Dataset 

```{r}
ds = read.csv("AB_NYC_2019.csv")
head(ds)
summary(ds)
```

# Data cleaning 

## Check for NA and NULL values
```{r}

#Check for NA
apply(ds,2,function(x) sum(is.na(x)))

# NOTES
# Remove NA, empty
#
#
#
#

```
```{r}
library(ggplot2)
library(ggpubr)
library(png)


img <- readPNG("map.png")
   

map_ds = ggplot() + background_image(img)+ geom_point(data = ds,  aes(y=latitude,x = longitude, color = price)) 

 
map_ds

```

## Normalisation and selection of the variables

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(dplyr)
library(tidyverse)
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}


clean_data = function(ds, pr=NULL, region= NULL, room = NULL)
{
  ds =ds[c(5,7:10)]
  ds=   ds%>%filter( price >= 15 )

  if(is.null(room))
  {
      ds$room_type = factor(ds$room_type, 
                        level= c("Private room","Entire home/apt","Shared room"), 
                        labels=c(1,2,3))
  }
  else
  {
     ds=   ds%>%filter( room_type==room)
     ds["room_type"] = NULL
  }
   if(is.null(region))
  {

    ds$neighbourhood_group = factor(ds$neighbourhood_group, 
                                  level= c("Brooklyn","Manhattan",
                                           "Queens","Staten Island", "Bronx"),
                                  labels=c(1,2,3,4,5))
   }
  else
  {
      ds=   ds%>%filter( price < pr & neighbourhood_group == region)
      ds$neighbourhood_group = NULL

  }
  if(!is.null(pr))
  {
    ds=   ds%>%filter( price < pr )

  }
  numerical = c("price")
  ds[numerical] = as.numeric(scale(ds[numerical]))

 
  return(ds)
}
#ggdraw() +
#  draw_image("New_York_City_.png") +
#  draw_plot(myplot)

dataset = clean_data(ds,500,"Manhattan",NULL)

head(dataset)
summary(dataset)
```

```{r}
library(ggplot2)
library(ggpubr)
library(png)


img <- readPNG("manhattan.png")
   

mappa = ggplot()+ geom_point(data = dataset,  aes(y=latitude,x = longitude, color = price)) 

 
mappa

```
# K-MEANS 

**x**: numeric matrix, numeric data frame or a numeric vector
**centers**: Possible values are the number of clusters (k) or a set of initial (distinct) cluster centers. If a number, a random set of (distinct) rows in x is chosen as the initial centers.
**iter.max**: The maximum number of iterations allowed. Default value is 10.
**nstart**: The number of random starting partitions when centers is a number. Trying nstart > 1 is often recommended.


```{r}
km.res = kmeans(dataset, 5, nstart = 25)

cat("First 10 Clusters association",km.res$cluster[1:10])
cat("\nCenters")
print(km.res$centers)
cat("\ntotss",km.res$totss)
cat("\nwithinss",km.res$withinss)
cat("\ntot_withinss",km.res$tot.withinss)

cat("\nbetweenss",km.res$betweenss)
cat("\nSize",km.res$size)
cat("\niter",km.res$iter)
cat("\nifault",km.res$ifault)

```

To create a beautiful graph of the clusters generated with the kmeans() function, will use the factoextra package.
```{r}
library(factoextra)

```

Cluster number for each of the observations
```{r}
head(km.res$cluster)
```

Cluster size
```{r}
km.res$size
```

Cluster means
```{r}
km.res$centers
```

```{r}
#dataset$neighbourhood_group = as.numeric( dataset$neighbourhood_group)
#dataset$room_type = as.numeric(  dataset$room_type)
fviz_cluster(km.res, data = dataset,
             palette = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"),
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot"
)

res <- hcut(dataset, k = 4, stand = FALSE)
fviz_dend(km.res, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"))

```



# Partitioning Around Medoids (PAM) ALGORITHM 
https://dpmartin42.github.io/posts/r/cluster-mixed-types

https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3
```{r}
library(cluster)
library(readr)
library(Rtsne)
```


Compute Gower distance
```{r}
dim(dataset)

#reduced <- dataset[ sample(1:nrow(dataset), nrow(dataset)/5) , ]

#print(dim(reduced))
gower_dist <- daisy(dataset, metric = "gower")
#proxy::simil(reduced, method = "gower")
#dim(gower_dist)

```

```{r}
hc1 <- hclust(gower_dist, method = "complete" )
hc1
plot(hc1, cex = 0.6, hang = -1)
```

```{r}

start.time <- Sys.time()
sil_width <- c(NA)
for(i in 2:8){  
  pam_fit <- pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}


end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")

plot(1:8, sil_width,
      xlab = "Number of clusters",
      ylab = "Silhouette Width")
lines(1:8, sil_width)

```

# FAMD

http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/115-famd-factor-analysis-of-mixed-data-in-r-essentials/
https://nextjournal.com/pc-methods/calculate-pc-mixed-data
https://cran.r-project.org/web/packages/FactoMineR/index.html
https://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont

```{r}

library("FactoMineR")
library("factoextra")
```

FAMD (base, ncp = 5, sup.var = NULL, ind.sup = NULL, graph = TRUE)
- base : a data frame with n rows (individuals) and p columns (variables).
- ncp: the number of dimensions kept in the results (by default 5)
- sup.var: a vector indicating the indexes of the supplementary variables.
- ind.sup: a vector indicating the indexes of the supplementary individuals.
- graph : a logical value. If TRUE a graph is displayed.

```{r}

res.famd <- FAMD(dataset, graph = FALSE, ncp = 10)
print(res.famd)
```


```{r}
eig.val <- get_eigenvalue(res.famd)
head(eig.val)

fviz_screeplot(res.famd)

```

```{r}
var <- get_famd_var(res.famd)
var
```

```{r}
# Coordinates of variables
head(var$coord)
# Cos2: quality of representation on the factore map
head(var$cos2)
# Contributions to the  dimensions
head(var$contrib)
```

```{r}
# Plot of variables
fviz_famd_var(res.famd, repel = TRUE)
# Contribution to the first dimension
fviz_contrib(res.famd, "var", axes = 1)
# Contribution to the second dimension
fviz_contrib(res.famd, "var", axes = 2)
# Contribution to the third dimension
fviz_contrib(res.famd, "var", axes = 3)
# Contribution to the forth dimension
fviz_contrib(res.famd, "var", axes = 4)
# Contribution to the fifth dimension
fviz_contrib(res.famd, "var", axes = 5)
# Contribution to the sixth dimension
fviz_contrib(res.famd, "var", axes = 6)
# Contribution to the seventh dimension
fviz_contrib(res.famd, "var", axes = 7)
# Contribution to the eighth dimension
fviz_contrib(res.famd, "var", axes = 8)
```

# PCAmixdata

```{r}

## Import library
library(PCAmixdata)

```

```{r}

## Split mixed dataset into quantitative and qualitative variables
## For now excluding the target variable "Churn", which will be added later as a supplementary variable
#split <- splitmix(dataset[1:5])  
split = splitmix(dataset)
## PCA
res.pcamix <- PCAmix(X.quanti=split$X.quanti,  
                     X.quali=split$X.quali, 
                     rename.level=TRUE, 
                     graph=FALSE, 
                     ndim=25)

res.pcamix


```

```{r}

## Inspect principal components
res.pcamix$eig
```

```{r}

# Use Scree Diagram to select the components:
plot(res.pcamix$eig, type="b", main="Scree Diagram", xlab="Number of Component", ylab="Eigenvalues")
abline(h=1, lwd=3, col="red")
```


# Hierarchical Cluster Analysis



```{r}
library(cluster)    # clustering algorithms
library(dendextend) # for comparing two dendrograms

```


```{r}
#reduced <- dataset[ sample(1:nrow(dataset), nrow(dataset)/10 ) , ]
#d <- dist(dataset, method = "euclidean")


# Hierarchical clustering using Complete Linkage
hc1 <- hclust(gower_dist, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
plot(cut(hc1, h=75)$upper, 
     main="Second branch of lower tree with cut at h=75")
```

```{r}
# methods to assess
#m <- c( "average", "single", "complete", "ward")
#names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
#ac <- function(x) {
#  agnes(reduced, method = x)$ac
#}

#library(purrr)
#map_dbl(m, ac)

```

```{r}
hc3 <- agnes(dataset, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 

```

# Clust miX type
k-prototypes in RAn implementation of the k-prototypes algorithm is given by the function

kproto(x, k, lambda = NULL, iter.max = 100, nstart = 1, na.rm = TRUE)

where
•x is a data frame with both numeric and factor variables.  As opposed to other existing Rpackages, the factor variables do not need to be preprocessed in advance and the order of thevariables does not matter.
•k is the number of clusters which has to be pre-specified. Alternatively, it can also be a vectorof observation indices or a data frame of prototypes with the same columns asx.  If ever atthe initialization or during the iteration process identical prototypes do occur, the number ofclusters will be reduced accordingly.

•lambda>0 is a real valued parameter that controls the trade off between Euclidean distancefor numeric variables and simple matching distance for factor variables for cluster assignment.If noλis specified the parameter is set automatically based on the data and a heuristic usingthe functionlambdaest(). Alternatively, a vector of lengthncol(x)can be passed tolambda(cf.Section on Extensions to the original algorithm).

•iter.maxsets the maximum number of iterations, just as inkmeans(). The algorithm may stopprior toiter.maxif no observations swap clusters.

•nstartmay be set to a value>1 to run k-prototypes multiple times. Similar to k-means, theresult of k-prototypes depends on its initialization. Ifnstart>1, the best solution (i.e. the onethat minimizesE) is returned.

•Generally, the algorithm can deal with missing data but as a defaultNAs are removed byna.rm= TRUE

```{r}
library(clustMixType)

kp = kmeans(dataset[-3],3)
kp 

#ssummary(kp)
```
```{r}
kp$cluster[1:50]

```

```{r}
library(ggplot2)
library(ggpubr)
library(png)


img <- readPNG("map.png")
   
index1 = which(kp$cluster==1)
index2 = which(kp$cluster==2)
index3= which(kp$cluster==3)
index4= which(kp$cluster==4)
index5= which(kp$cluster==5)
index6 = which(kp$cluster==6)
index7 = which(kp$cluster==7)
index8= which(kp$cluster==8)
index9= which(kp$cluster==9)
index10= which(kp$cluster==10)

clust1 = dataset[index1,]
clust2 = dataset[index2,]
clust3 = dataset[index3,]
clust4 = dataset[index4,]
clust5 = dataset[index5,]
clust6 = dataset[index6,]
clust7 = dataset[index7,]
clust8 = dataset[index8,]
clust9 = dataset[index9,]
clust10 = dataset[index10,]

myplot= ggplot() +background_image(img)+
      geom_point(data = clust3, aes(y = latitude, x = longitude), color= "darkgreen") +
geom_point(data = clust1, aes(y = latitude, x = longitude), color = "red") +
  geom_point(data = clust2, aes(y = latitude, x = longitude), color= "blue") +
  geom_point(data = clust4, aes(y = latitude, x = longitude), color= "purple") +
    geom_point(data =clust5, aes(y = latitude, x = longitude), color= "orange") +
geom_point(data = clust6, aes(y = latitude, x = longitude), color = "black") +
  geom_point(data = clust7, aes(y = latitude, x = longitude), color= "purple") +
    geom_point(data = clust8, aes(y = latitude, x = longitude), color= "pink") +
  geom_point(data = clust9, aes(y = latitude, x = longitude), color= "gray") +
    geom_point(data = clust10, aes(y = latitude, x = longitude), color= "cyan") +
  xlab('data_date') +
  ylab('percent.change')

#size = price
mappa = ggplot() + background_image(img)+ geom_point(data = dataset,  aes(y=latitude,x = longitude, color = price)) 

 
mappa

#dev.new(width=5, height=4)
myplot+ theme(plot.margin = unit(c(1,1,1,1),"cm"),legend.title = element_text(colour="blue", size=10, 
                                      face="bold"))
 
```

```{r}

cat("\nCluster1:\n")
summary(clust1)

cat("\nCluster2:\n")
summary(clust2)


cat("\nCluster3:\n")
summary(clust3)

cat("\nCluster4:\n")
summary(clust4)

cat("\nCluster5:\n")
summary(clust5)

cat("\nCluster6:\n")
summary(clust6)

cat("\nCluster7:\n")
summary(clust7)

cat("\nCluster8:\n")
summary(clust8)

cat("\nCluster9:\n")
summary(clust9)

cat("\nCluster10s:\n")
summary(clust10)
```

```{r}

```

```{r}

library(png)
my_image <- readPNG("newyork.png")
 
# Set up a plot area with no plot
plot(1:2, type='n', main="", xlab="x", ylab="y")
 
# Get the plot information so the image will fill the plot box, and draw it
lim <- par()
rasterImage(my_image, 
            xleft=1, xright=2, 
            ybottom=1, ytop=2)
grid()
 
points(clust1$latitude,clust1$longitude, col = "black", lwd = 5, type="b")
#Add your plot !
#lines(
#  x=c(1, 1.2, 1.4, 1.6, 1.8, 2.0), 
#  y=c(1, 1.3, 1.7, 1.6, 1.7, 1.0), 
#  type="b", lwd=5, col="black")
```

install.packages(c("cowplot", "googleway", "ggplot2", "ggrepel", 
"ggspatial", "libwgeom", "sf", "rnaturalearth", "rnaturalearthdata")

We start by loading the basic packages necessary for all maps, i.e. ggplot2 and sf. We also suggest to use the classic dark-on-light theme for ggplot2 (theme_bw), which is appropriate for maps:


```{r}
# load libraries
library(sf)
library(raster)
library(dplyr)
library(spData)
library(tmap)    # for static and interactive maps
library(leaflet) # for interactive maps
library(ggplot2) # tidyverse data visualization package
library(shiny)   # for web applications
#library(spDataLarge)



```
https://rpubs.com/jhofman/nycmaps   !!!!

```{r}

library(wesanderson)
par(mfrow=c(2,2))
clprofiles(kp, dataset[-1], col = wes_palette("Royal1",5, type = "continuous"))  
```

```{r}
Es = numeric(10)
for(i in 1:10)
  {
    kpres <- kproto(dataset, k = i, nstart = 5)
    Es[i] <- kpres$tot.withinss
  }
plot(1:10, Es, type = "b", ylab = "Objective Function", xlab = "# Clusters",main = "Scree Plot") 
```



```{r}
d <- dist(dataset, method = "euclidean") # distance matrix
fit <- hclust(d, method="complete")
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters
rect.hclust(fit, k=5, border="red") 
```


```{r}
fit <- kmeans(dataset, 5)

# Cluster Plot against 1st 2 principal components

# vary parameters for most readable graph
library(cluster)
clusplot(dataset, fit$cluster, color=TRUE, shade=TRUE,
   labels=2, lines=0)

# Centroid Plot against 1st 2 discriminant functions
library(fpc)
plotcluster(dataset, fit$cluster) 
```