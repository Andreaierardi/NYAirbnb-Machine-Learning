---
title: "Statistical Learning Project - Supervised Learning"
output:
  pdf_document: default
  html_document: default
---

```{r}
#https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data
library(knitr)
library(ggplot2)
library(tidyr)
library(dplyr)

```

# Read Dataset 

```{r}
ds = read.csv("AB_NYC_2019.csv")
head(ds)
summary(ds)
```

# Data Inspection

```{r}
library(png)
library(ggpubr)
img <- readPNG("map.png")
   

map_ds = ggplot() + background_image(img)+ geom_point(data = ds,  aes(y=latitude,x = longitude, color = price)) 
map_ds

```


# Data cleaning 

## Check for NA and NULL values
```{r}

#Check for NA
apply(ds,2,function(x) sum(is.na(x)))


```


## Normalisation and selection of the variables

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(dplyr)
library(tidyverse)
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}


clean_data = function(ds, pr=NULL, region= NULL, room = NULL)
{
  ds =ds[c(5,7:10)]
  ds=   ds%>%filter( price >= 15 )

  if(is.null(room))
  {
      ds$room_type = factor(ds$room_type, 
                        level= c("Private room","Entire home/apt","Shared room"), 
                        labels=c(1,2,3))
  }
  else
  {
     ds=   ds%>%filter( room_type==room)
     ds["room_type"] = NULL
  }
   if(is.null(region))
  {

    ds$neighbourhood_group = factor(ds$neighbourhood_group, 
                                  level= c("Brooklyn","Manhattan",
                                           "Queens","Staten Island", "Bronx"),
                                  labels=c(1,2,3,4,5))
   }
  else
  {
      ds=   ds%>%filter( price < pr & neighbourhood_group == region)
      ds$neighbourhood_group = NULL

  }
  if(!is.null(pr))
  {
    ds=   ds%>%filter( price < pr )

  }
  numerical = c("price")
  ds[numerical] = as.numeric(scale(ds[numerical]))

 
  return(ds)
}
#ggdraw() +
#  draw_image("New_York_City_.png") +
#  draw_plot(myplot)

dataset = clean_data(ds,500,NULL,NULL)

head(dataset)
summary(dataset)
```

```{r}

mappa = ggplot()+ geom_point(data = dataset,  aes(y=latitude,x = longitude, color = price)) 

 
mappa

```

# Split data into train and test sets

```{r}



library(caTools)
library(caret)

data_clean = dataset
sample = sample.split(data_clean, SplitRatio = .75)
train = subset(data_clean, sample == TRUE)
test  = subset(data_clean, sample == FALSE)

print("Initial data shape")
print(dim(data_clean))
print("Train shape")
print(dim(train))
head(train)
print("Test shape")
print(dim(test))
head(test)
```

```{r}
library(dplyr)
ds_neig =  dataset %>% filter(neighbourhood_group == 2)
ds_neig = ds_neig[-1]


sample_neig = sample.split(ds_neig, SplitRatio = .75)
train_neig = subset(ds_neig, sample == TRUE)
test_neig  = subset(ds_neig, sample == FALSE)


ds_neig_room = dataset %>% filter(neighbourhood_group == 2 & room_type == 1)
ds_neig_room = ds_neig_room[-1][-3]

sample_neig = sample.split(ds_neig, SplitRatio = .75)
train_neig_room = subset(ds_neig_room, sample == TRUE)
test_neig_room  = subset(ds_neig_room, sample == FALSE)



train_neig
train_neig_room
```

# LINEAR REGRESSION

https://datascienceplus.com/fitting-neural-network-in-r/


```{r}

cat("=== Linear Regression without filters ===\n")
lm.fit <- lm(price~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$price)^2)/nrow(test)
cat("MSE: ",MSE.lm)

plot(lm.fit)
```
```{r}
library(dplyr)

cat(" === Linear Regression selecting the Neighboorhood group === \n")
cat("Neighboorhood group = Manhattan\n ")
lm2.fit <- lm(price~., data=train_neig)
summary(lm2.fit)
pr2.lm <- predict(lm2.fit,test_neig)
MSE2.lm <- sum((pr2.lm - test_neig$price)^2)/nrow(test_neig)

cat("MSE: ",MSE2.lm)
plot(lm2.fit)
```

```{r}
library(dplyr)

cat("=== Linear Regression selecting the Neighboorhood group and room_type ===\n")
cat("Neighboorhood group = Manhattan and room_type = Entire home/apt\n ")

lm3.fit <- lm(price~., data=train_neig_room)
summary(lm3.fit)
pr3.lm <- predict(lm3.fit,test_neig_room)
MSE3.lm <- sum((pr3.lm - test_neig_room$price)^2)/nrow(test_neig_room)

cat("MSE: ",MSE3.lm)
plot(lm3.fit)
```

```{r}
par(mfrow=c(1,2))
#plot(test$price,nn_pred$net.result,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
#abline(0,1,lwd=2)
#legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$price,pr.lm,col='red',main='Real vs predicted LN',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='red', bty='n', cex=.95)
```


# DECISION  TREE
  
library(rpart)
library(caret)
library(e1071)


dt = train( price~ ., data = train,method = "rpart")
pred2 = predict(dt, data = test$price)
table(pred2, test$price)

```{r}
library(tree)

library(MASS)
tree_reg =tree(price~., data = train)
cat(" === Decision Tree without filters === \n")
summary(tree_reg)

yhat=predict(tree_reg,test)
tree_reg_mse = mean((yhat-test$price)^2)
tree_reg_mse

plot(tree_reg)
text(tree_reg,pretty=0)


```

```{r}

cat(" === Decision Tree selecting the Neighboorhood group === \n")
cat("Neighboorhood group = Manhattan\n ")

tree_reg =tree(price~., data = train_neig)
summary(tree_reg)
tree_reg_mse = mean((predict(tree_reg,test_neig)-test_neig$price)^2)
tree_reg_mse


plot(tree_reg)
text(tree_reg,pretty=1)


```


```{r}
cat("=== Decision Tree selecting the Neighboorhood group and room_type ===\n")
cat("Neighboorhood group = Manhattan and room_type = Entire home/apt\n ")
tree_reg =tree(price~., data = train_neig_room)
summary(tree_reg)
tree_reg_mse = mean((predict(tree_reg,test_neig_room)-test_neig_room$price)^2)
tree_reg_mse

plot(tree_reg)
text(tree_reg,pretty=0)


```

```{r}
test$price = as.numeric(test$price)
tree_reg.pred=predict(tree_reg, test)
#table(tree_reg.pred,test$price)

par(mfrow=c(1,2))
plot(as.numeric(test$price),tree_reg.pred,col='red',main='Real vs predicted Tree Regressor',pch=18,cex=0.7)
legend('bottomright',legend='TR',pch=18,col='red', bty='n')
```

```{r}


yhat=predict(tree_reg,test)
plot(yhat,test$price)
abline(0,1)
tree_reg_mse = mean((yhat-test$price)^2)
tree_reg_mse
```

# RANDOM FOREST

https://www.guru99.com/r-random-forest-tutorial.html


GOOD VIDEO FOR PARAMETER OPTIMISATION
https://www.youtube.com/watch?v=6EXPYzbfLCE

TUNING 
https://uc-r.github.io/random_forests



```{r}
library(randomForest)

cat(" === Random Forest without filters === \n")
rf <- randomForest(
  price ~ . ,
  data=train,
)
rf
varImpPlot(rf)
plot(rf)
```

```{r}

cat("=== Random Forest selecting the Neighboorhood group and room_type ===\n")
cat("Neighboorhood group = Manhattan and room_type = Entire home/apt\n ")

rf2 <- randomForest(
  price ~ .,
  data=train_neig,
  # importance = TRUE
)
rf2
varImpPlot(rf2)
plot(rf2)
```

```{r}
cat(" === Random Forest selecting the Neighboorhood group === \n")
cat("Neighboorhood group = Manhattan\n ")


rf3 <- randomForest(
  price ~ .,
  data=train_neig_room,
  # importance = TRUE
)
rf3
varImpPlot(rf3)
plot(rf3)

```


```{r}
par(mfrow=c(1,2))
plot(test$price,predict(rf,test),col='red',main='Real vs predicted RF',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')


par(mfrow=c(1,2))
plot(train$price,predict(rf),col='red',main='OOB error',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
```
```{r}

pred <-predict(rf,test)
actuals_preds <- data.frame(cbind(actuals=test$price, predicteds=pred))  # make actuals_predicteds dataframe.

correlation_accuracy <- cor(actuals_preds)  # 0.6043436
print(correlation_accuracy)
head(actuals_preds)
```
```{r}
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))  
min_max_accuracy

mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
mape
```

```{r}
library(randomForest)
library(e1071)
library(caret)
trControl <- trainControl(method = "cv",
                          number = 10,
                          search = "grid",
                          allowParallel = TRUE
                        )
trControl$method
trControl$number
```


```{r}
library(doParallel)
library(doFuture)

start.time <- Sys.time()


rf_default <- caret::train(
  price ~ . ,
  data=train_neig_room,
  method = "rf",
  trControl = trControl,
  num.threads = availableCores() # <- This one
  
)

end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken

print("")
print("-- RANDOM FOREST -- ")
print(rf_default)


```


```{r}

start.time <- Sys.time()

set.seed(1234)
tuneGrid <- expand.grid(.mtry = c(1: 10))
rf_mtry <- caret::train(
  price~.,
  data = train_neig_room,
  method = "rf",
  #metric = "Accuracy",
  tuneGrid = tuneGrid,
  trControl = trControl,
  importance = TRUE,
  nodesize = 14,
  ntree = 300,
  num.threads = availableCores()
)


end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")


print(rf_mtry)
```

```{r}
best_mtry <- rf_mtry$bestTune$mtry 
cat("Best mtry value:",best_mtry)
cat("\nMax accuracy mtry:", max(rf_mtry$results$Accuracy))


```




```{r}

start.time <- Sys.time()

store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in c(5: 15)) {
  set.seed(1234)
  rf_maxnode <- caret::train(price~.,
                      data = train_neig_room,
                      method = "rf",
                      #metric = "Accuracy",
                      tuneGrid = tuneGrid,
                      trControl = trControl,
                      importance = TRUE,
                      nodesize = 14,
                      maxnodes = maxnodes,
                      ntree = 300,
  )
  current_iteration <- toString(maxnodes)
  store_maxnode[[current_iteration]] <- rf_maxnode
}

end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")



results_mtry <- resamples(store_maxnode)
summary(results_mtry)
```

```{r}

start.time <- Sys.time()

for (maxnodes in c(15 : 30)) {
  set.seed(1234)
  rf_maxnode <- caret::train(price~.,
                      data = train_neig_room,
                      method = "rf",
                     # metric = "Accuracy",
                      tuneGrid = tuneGrid,
                      trControl = trControl,
                      importance = TRUE,
                      nodesize = 14,
                      maxnodes = maxnodes,
                      ntree = 300)
  current_iteration <- toString(maxnodes)
  store_maxnode[[current_iteration]] <- rf_maxnode
}

end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")

results_mtry <- resamples(store_maxnode)
summary(results_mtry)
```
Result: 30


## Step 4) Search the best ntrees
Now that you have the best value of mtry and maxnode, 
you can tune the number of trees. The method is exactly the same as maxnode.

```{r}


start.time <- Sys.time()

store_maxtrees <- list()
for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000)) {
  set.seed(5678)
  rf_maxtrees <- caret::train(price~.,
                       data = train_neig_room,
                       method = "rf",
                       tuneGrid = tuneGrid,
                       trControl = trControl,
                      # importance = TRUE,
                       nodesize = 14,
                       maxnodes = 23,
                       ntree = ntree)
  key <- toString(ntree)
  store_maxtrees[[key]] <- rf_maxtrees
}

end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")                                                          

results_tree <- resamples(store_maxtrees)
summary(results_tree)
```


## Step 5) Evaluate the model
```{r}
fit_rf <- caret::train(price~.,
                train_neig_room,
                method = "rf",
                tuneGrid = tuneGrid,
                trControl = trControl,
                importance = TRUE,
                nodesize = 14,
                ntree = 800,
                maxnodes = 30)



prediction <-predict(fit_rf, test)

#prediction_default <-caret::predict(rf, test)

plot(test$price,prediction)
print(fit_rf)
#varImpPlot(fit_rf)
#plot(prediction_default, test$price)

```

```{r}
## Step 6) Visualize Result
ele = randomForest::randomForest(
  x =  train_neig_room[-3],
  y = train_neig_room$price,
  xtest = test_neig_room[-3],
  ytest = test_neig_room$price,
  mtry = 5,
  maxnodes=30,
  ntree = 800
)
print(ele)
varImpPlot(ele)

plot(ele)
```


# RANGER RANDOM FOREST
```{r}


library(ranger)
library(tuneRanger)
rangerReg <- ranger( price~ ., data = train, write.forest = TRUE, classification = F)
rangerReg
rangerReg_pred = predict(rangerReg, data = test)
rangerReg_pred

library(plotly)
plot_ly(x = test$price, y = predictions(rangerReg_pred))
```
```{r}


library(ranger)
library(tuneRanger)
rangerReg <- ranger( price~ ., data = train_neig, write.forest = TRUE, classification = F)
rangerReg
rangerReg_pred = predict(rangerReg, data = test_neig)
rangerReg_pred

library(plotly)
plot_ly(x = test_neig$price, y = predictions(rangerReg_pred))
```

```{r}



rangerReg <- ranger( price~ ., data = train_neig_room, write.forest = TRUE, classification = F)
rangerReg
rangerReg_pred = predict(rangerReg, data = test_neig_room)
rangerReg_pred

library(plotly)
plot_ly(x = test_neig_room$price, y = predictions(rangerReg_pred))
```



## RANGER TUNING

```{r}
library(tuneRanger)

# https://github.com/PhilippPro/tuneRanger
# https://mlr.mlr-org.com/articles/tutorial/measures.html
task = makeRegrTask(data = train, target = "price")
estimateTimeTuneRanger(task, iters = 20, num.threads = 8, num.trees = 1000)


start.time <- Sys.time()


     
res = tuneRanger(task, measure = list(mse), num.trees = 1000, 
                 num.threads = 8, iters = 20,  show.info = getOption("mlrMBO.show.info", TRUE))



end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("") 
```
```{r}
library(tuneRanger)

cat(" === Random Forest selecting the Neighboorhood group === \n")
cat("Neighboorhood group = Manhattan\n ")

task2 = makeRegrTask(data = train_neig, target = "price")
estimateTimeTuneRanger(task2, iters = 20, num.threads = 8, num.trees = 1000)


start.time <- Sys.time()


     
res2 = tuneRanger(task2, measure = list(mse), num.trees = 1000, 
                 num.threads = 8, iters = 20,  show.info = getOption("mlrMBO.show.info", TRUE))



end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")
```

```{r}
library(tuneRanger)

cat(" === Ranger Random Forest selecting the Neighboorhood group and room_type=== \n")
cat("Neighboorhood group = Manhattan and room_type=Entire-home/Apt\n ")

task3 = makeRegrTask(data = train_neig_room, target = "price")
estimateTimeTuneRanger(task3, iters = 20, num.threads = 8, num.trees = 1000)


start.time <- Sys.time()


     
res3 = tuneRanger(task3, measure = list(mse), num.trees = 1000, 
                 num.threads = 8, iters = 20,  show.info = getOption("mlrMBO.show.info", TRUE))



end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")  
```

Mean of best 5 % of the results
```{r}
res$model

```


Recommended parameter settings: 
  mtry min.node.size sample.fraction
1    2            55	0.2136541
Results: 
  mse exec.time
1 0.933998	2.756667	


Model with the new tuned hyperparameters
```{r}
res2$model
```

Model with the new tuned hyperparameters
```{r}
res3$model
```

```{r}
tuned_rangerReg <- ranger( price~ ., data = train, write.forest = TRUE, classification = F, mtry= 3, 
                           min.node.size = 35	, sample.fraction = 0.326,num.trees = 1000, replace= FALSE)
tuned_rangerReg
tuned_rangerReg_pred = predict(tuned_rangerReg, data = test)
tuned_rangerReg_pred
```

```{r}
#Hyperparameters: num.threads=8,verbose=FALSE,respect.unordered.factors=order,
#mtry=2,min.node.size=22,sample.fraction=0.231,num.trees=1e+03,replace=FALSE

tuned_rangerReg2 <- ranger( price~ ., data = train_neig, write.forest = TRUE, classification = F, mtry= 2, 
                           min.node.size = 22, sample.fraction = 0.231,num.trees = 1000, replace= FALSE)
tuned_rangerReg2
tuned_rangerReg_pred2 = predict(tuned_rangerReg2, data = test_neig)
tuned_rangerReg_pred2

```


```{r}

#Hyperparameters: num.threads=8,verbose=FALSE,respect.unordered.factors=order,mtry=1,min.node.size=5,sample.fraction=0.21,num.trees=1e+03,replace=FALSE

tuned_rangerReg3 <- ranger( price~ ., data = train_neig_room, write.forest = TRUE, classification = F, mtry=1, 
                           min.node.size = 5	, sample.fraction = 0.21,num.trees = 1000, replace= FALSE)
tuned_rangerReg3
tuned_rangerReg_pred3 = predict(tuned_rangerReg3, data = test_neig_room)
tuned_rangerReg_pred3
```

# NEURAL NETWORKS 


https://medium.com/@brscntyz/neural-network-in-r-e275302b6e44
https://datascienceplus.com/fitting-neural-network-in-r/
https://www.kdnuggets.com/2016/08/begineers-guide-neural-networks-r.html/2
```{r}
library(ISLR)
library(tidyverse)
library("keras")
library(neuralnet)
library(Hmisc)
```

```{r}
m <- model.matrix( 
  ~price+neighbourhood_group+room_type+longitude+latitude,
  data = train 
)

m_test  <- model.matrix( 
  ~price+neighbourhood_group+room_type+longitude+latitude,
  data = test 
)

head(m)
```

```{r}
#nn=neuralnet(price~ neighbourhood_group2+ neighbourhood_group3 +neighbourhood_group4+ neighbourhood_group5+ room_type2+ room_type3+longitude+latitude,data=m, hidden=10,act.fct = "logistic",
#             linear.output = TRUE,stepmax=10^5,threshold = 0.01)

```

Also you can change your hidden layers by specifiying with numbers in vector like this
```{r}
#nn=neuralnet( price~ neighbourhood_group2+ neighbourhood_group3 +neighbourhood_group4+ neighbourhood_group5+ room_type2+ room_type3+longitude+latitude,data=m, hidden=c(7,6,5),act.fct = "logistic",
#           linear.output = TRUE,stepmax=10^5,threshold = 0.01)

#hidden=c(7,6,5)
```



Then, prediction and calculation of error comes. I calculate the error with Root mean error method.

nn_pred=compute(nn,test[,1:13])
nn_pred$net.resultRMSE <- function(actual,predicted) {
  return(sqrt(sum(actual^2-predicted^2)/length(actual)))
}
summary(nn_pred)
nn_pred <- is.numeric(nn_pred)
RMSE(test$price,nn_pred)

plot(test$price,nn_pred$net.result)

pr.nn_ <- nn_pred$net.result*(max(dataset$price)-min(dataset$price))+min(dataset$price)


# NEURAL NETWORKS WITH KERAS

!
https://www.datatechnotes.com/2019/01/regression-example-with-keras-in-r.html
https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/
!


For regression : https://keras.rstudio.com/articles/tutorial_basic_regression.html

For classification: https://keras.rstudio.com/
```{r}
library(keras)
library(dplyr)

#needed: 
# - tensorflow::install_tensorflow()
# - miniconda

nntrain = train
nntest = test
nntrain$room_type = keras::to_categorical(nntrain$room_type)
nntrain$neighbourhood_group = keras::to_categorical(nntrain$neighbourhood_group)
nntest$room_type = keras::to_categorical(nntest$room_type)
nntest$neighbourhood_group = keras::to_categorical(nntest$neighbourhood_group)

target = as.vector(nntrain$price)
feat = as.matrix(as_tibble(nntrain[-5]))
target_test =as.vector(nntest$price)
feat_test =as.matrix(as_tibble(nntest[-5]))

```

```{r}

model = keras_model_sequential() %>% 
   layer_dense(units=64, activation="relu", input_shape=12) %>% 
   layer_dense(units=32, activation = "relu") %>% 
   layer_dense(units=1, activation="linear")
 
model %>% compile(
   loss = "mse",
   optimizer =  "adam", 
   metrics = list("mean_absolute_error")
 )
 
model %>% summary()

```
```{r}

print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)    

model <- build_model()

history <- model %>% fit(
  x = feat,
  y = target,
  epochs = 500,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(print_dot_callback)
)
```
```{r}

plot(history)


plot(history, metrics = "mean_absolute_error", smooth = T) +
  coord_cartesian(ylim = c(0, 5))
```


```{r}
eva = model %>% evaluate(feat_test,target_test, verbose = 0)

mae = eva[1]
loss = eva[2]
paste0("Mean absolute error on test set: ", mae)
paste0("Loss: ", loss)


```

```{r}

y_pred = model %>% predict(feat_test)

x_axes = seq(1:length(y_pred))
plot(x_axes, target_test, type="l", col="red")
lines(x_axes, y_pred, col="blue")
legend("topleft", legend=c("y-original", "y-predicted"),
        col=c("red", "blue"), lty=1,cex=0.8)



MSE.nn = mean((y_pred-target_test)^2)
MSE.nn

```



```{r}
build_model <- function() {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
                input_shape = 12) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1)
  
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )
  
  model
}

model <- build_model()
model %>% summary()
```

```{r}
# Display training progress by printing a single dot for each completed epoch.

print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)    

start.time = Sys.time()
epochs <- 500

# Fit the model and store training stats
history <- model %>% fit(
  feat,
  target,
  epochs = epochs,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(print_dot_callback)
)


end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")
```

```{r}

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
epochs <- 500

start.time <- Sys.time()

model <- build_model()
history <- model %>% fit(
  feat,
  target,
  epochs = epochs,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(early_stop, print_dot_callback)
)


end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")


history$params$epochs =length(history$metrics$mean_absolute_error)

plot(history, metrics = "mean_absolute_error", smooth = T) +
  coord_cartesian(xlim = c(0, history$params$epochs*1.2), ylim = c(0, 1.5))


```





```{r}

eva = model %>% evaluate(feat_test,target_test, verbose = 0)

mae = eva[1]
loss = eva[2]
paste0("Mean absolute error on test set: ", mae)
paste0("Loss: ", loss)


```

```{r}

y_pred = model %>% predict(feat_test)

x_axes = seq(1:length(y_pred))
plot(x_axes, target_test, type="l", col="red")
lines(x_axes, y_pred, col="blue")
legend("topleft", legend=c("y-original", "y-predicted"),
        col=c("red", "blue"), lty=1,cex=0.8)



MSE.nn = mean((y_pred-target_test)^2)
MSE.nn

```






```{r}
library(ggplot2)

plot(history)
plot(history, metrics = "mean_absolute_error", smooth = T) +
  coord_cartesian(ylim = c(0, 5))
```

































# K-MEANS 

**x**: numeric matrix, numeric data frame or a numeric vector
**centers**: Possible values are the number of clusters (k) or a set of initial (distinct) cluster centers. If a number, a random set of (distinct) rows in x is chosen as the initial centers.
**iter.max**: The maximum number of iterations allowed. Default value is 10.
**nstart**: The number of random starting partitions when centers is a number. Trying nstart > 1 is often recommended.


# Clust miX type
k-prototypes in RAn implementation of the k-prototypes algorithm is given by the function

kproto(x, k, lambda = NULL, iter.max = 100, nstart = 1, na.rm = TRUE)

where
•x is a data frame with both numeric and factor variables.  As opposed to other existing Rpackages, the factor variables do not need to be preprocessed in advance and the order of thevariables does not matter.
•k is the number of clusters which has to be pre-specified. Alternatively, it can also be a vectorof observation indices or a data frame of prototypes with the same columns asx.  If ever atthe initialization or during the iteration process identical prototypes do occur, the number ofclusters will be reduced accordingly.

•lambda>0 is a real valued parameter that controls the trade off between Euclidean distancefor numeric variables and simple matching distance for factor variables for cluster assignment.If noλis specified the parameter is set automatically based on the data and a heuristic usingthe functionlambdaest(). Alternatively, a vector of lengthncol(x)can be passed tolambda(cf.Section on Extensions to the original algorithm).

•iter.maxsets the maximum number of iterations, just as inkmeans(). The algorithm may stopprior toiter.maxif no observations swap clusters.

•nstartmay be set to a value>1 to run k-prototypes multiple times. Similar to k-means, theresult of k-prototypes depends on its initialization. Ifnstart>1, the best solution (i.e. the onethat minimizesE) is returned.

•Generally, the algorithm can deal with missing data but as a defaultNAs are removed byna.rm= TRUE

```{r}
library(clustMixType)

clust_num = 5

get_clusters = function(dts, num, factors=T)
{
  l = list()
  if(isFALSE(factors))
  {
    l$cl = kmeans(dts,num)
  }
  else
  {
      l$cl = kproto(dts,num)

  }
  
  clust = list()
  
  for (i in 1:num)
  {
    indexes = l$cl$cluster == i
    clust[[i]] = dts[indexes,]
  }

  myplot= ggplot() +background_image(img)+  xlab('data_date') +  ylab('percent.change')+ theme(plot.margin = unit(c(1,1,1,1),"cm"),legend.title = element_text(colour="blue", size=10, face="bold"))
  count = 1
  for(el in clust)
  {
    myplot = myplot+geom_point(data = el, aes(y = latitude, x = longitude),color= count) 
    plot(ggplot()+geom_point(data = el, aes(y = latitude, x = longitude),color= count) )
    print(paste0("=== clust: ",count,"==="))
    print(summary(el))
    print("=========")

    count= count+1
  }
  l$myplot = myplot
  return(l)
}
lis = get_clusters(dataset, clust_num)
lis$myplot

lis2 = get_clusters(ds_neig,5)
lis2$myplot


lis3 = get_clusters(ds_neig_room,5,F)
lis3$myplot

```


```{r}

library(wesanderson)
par(mfrow=c(2,2))
clprofiles(lis$cl, dataset, col = wes_palette("Royal1",5, type = "continuous"))  
```

```{r}
Es = numeric(10)
for(i in 1:10)
  {
    kpres <- kproto(dataset, k = i, nstart = 5)
    Es[i] <- kpres$tot.withinss
  }
plot(1:10, Es, type = "b", ylab = "Objective Function", xlab = "# Clusters",main = "Scree Plot") 
```



d <- dist(dataset, method = "euclidean") # distance matrix
fit <- hclust(d, method="complete")
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters
rect.hclust(fit, k=5, border="red") 


```{r}
km.res <- kmeans(ds_neig_room, 5)

# Cluster Plot against 1st 2 principal components

# vary parameters for most readable graph
library(cluster)
clusplot(ds_neig_room, km.res$cluster, color=TRUE, shade=TRUE,
   labels=2, lines=0)

# Centroid Plot against 1st 2 discriminant functions
library(fpc)
plotcluster(ds_neig_room, km.res$cluster) 
```




To create a beautiful graph of the clusters generated with the kmeans() function, will use the factoextra package.
```{r}
library(factoextra)

```

Cluster number for each of the observations
```{r}
head(km.res$cluster)
```

Cluster size
```{r}
km.res$size
```

Cluster means
```{r}
km.res$centers
```

```{r}

fviz_cluster(km.res, data = ds_neig_room,
             palette = c("#00AFBB","#2E9FDF","#0000FF", "#E7B800", "#FC4E07"),
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot"
)

res <- hcut(ds_neig_room, k = 5, stand = FALSE)
fviz_dend(res, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB","#2E9FDF", "#0000FF","#E7B800", "#FC4E07"))

```



# Partitioning Around Medoids (PAM) ALGORITHM 
https://dpmartin42.github.io/posts/r/cluster-mixed-types

https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3
```{r}
library(cluster)
library(readr)
library(Rtsne)
```


Compute Gower distance
```{r}
reduced = ds_neig =  dataset %>% filter(neighbourhood_group == 3)

#reduced <- dataset[ sample(1:nrow(dataset), nrow(dataset)/5) , ]

#print(dim(reduced))
gower_dist <- daisy(reduced, metric = "gower")
#proxy::simil(reduced, method = "gower")
#dim(gower_dist)

```

```{r}
hc1 <- hclust(gower_dist, method = "complete" )
hc1
plot(hc1, cex = 0.6, hang = -1)
```



start.time <- Sys.time()
sil_width <- c(NA)
for(i in 2:8){  
  pam_fit <- pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}


end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")

plot(1:8, sil_width,
      xlab = "Number of clusters",
      ylab = "Silhouette Width")
lines(1:8, sil_width)


# FAMD

http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/115-famd-factor-analysis-of-mixed-data-in-r-essentials/
https://nextjournal.com/pc-methods/calculate-pc-mixed-data
https://cran.r-project.org/web/packages/FactoMineR/index.html
https://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont

```{r}

library("FactoMineR")
library("factoextra")
```

FAMD (base, ncp = 5, sup.var = NULL, ind.sup = NULL, graph = TRUE)
- base : a data frame with n rows (individuals) and p columns (variables).
- ncp: the number of dimensions kept in the results (by default 5)
- sup.var: a vector indicating the indexes of the supplementary variables.
- ind.sup: a vector indicating the indexes of the supplementary individuals.
- graph : a logical value. If TRUE a graph is displayed.

```{r}

res.famd <- FAMD(dataset, graph = FALSE, ncp = 10)
print(res.famd)
```


```{r}
eig.val <- get_eigenvalue(res.famd)
head(eig.val)

fviz_screeplot(res.famd)

```

```{r}
var <- get_famd_var(res.famd)
var
```

```{r}
# Coordinates of variables
head(var$coord)
# Cos2: quality of representation on the factore map
head(var$cos2)
# Contributions to the  dimensions
head(var$contrib)
```

```{r}
# Plot of variables
fviz_famd_var(res.famd, repel = TRUE)
# Contribution to the first dimension
fviz_contrib(res.famd, "var", axes = 1)
# Contribution to the second dimension
fviz_contrib(res.famd, "var", axes = 2)
# Contribution to the third dimension
fviz_contrib(res.famd, "var", axes = 3)
# Contribution to the forth dimension
fviz_contrib(res.famd, "var", axes = 4)
# Contribution to the fifth dimension
fviz_contrib(res.famd, "var", axes = 5)
# Contribution to the sixth dimension
fviz_contrib(res.famd, "var", axes = 6)
# Contribution to the seventh dimension
fviz_contrib(res.famd, "var", axes = 7)
# Contribution to the eighth dimension
fviz_contrib(res.famd, "var", axes = 8)
```

# PCAmixdata

```{r}

## Import library
library(PCAmixdata)

```

```{r}

## Split mixed dataset into quantitative and qualitative variables
## For now excluding the target variable "Churn", which will be added later as a supplementary variable
#split <- splitmix(dataset[1:5])  
split = splitmix(dataset)
## PCA
res.pcamix <- PCAmix(X.quanti=split$X.quanti,  
                     X.quali=split$X.quali, 
                     rename.level=TRUE, 
                     graph=FALSE, 
                     ndim=25)

res.pcamix


```

```{r}

## Inspect principal components
res.pcamix$eig
```

```{r}

# Use Scree Diagram to select the components:
plot(res.pcamix$eig, type="b", main="Scree Diagram", xlab="Number of Component", ylab="Eigenvalues")
abline(h=1, lwd=3, col="red")
```


# Hierarchical Cluster Analysis



```{r}
library(cluster)    # clustering algorithms
library(dendextend) # for comparing two dendrograms

```



#reduced <- dataset[ sample(1:nrow(dataset), nrow(dataset)/10 ) , ]
#d <- dist(dataset, method = "euclidean")


# Hierarchical clustering using Complete Linkage
hc1 <- hclust(gower_dist, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
plot(cut(hc1, h=75)$upper, 
     main="Second branch of lower tree with cut at h=75")

```{r}
# methods to assess
#m <- c( "average", "single", "complete", "ward")
#names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
#ac <- function(x) {
#  agnes(reduced, method = x)$ac
#}

#library(purrr)
#map_dbl(m, ac)

```


hc3 <- agnes(ds_neig_room, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 

