---
title: "Statistical Learning Project - Unsupervised Learning"
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```



```{r}
#https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data
library(ggplot2)
library(ggmap)
library(tidyr)
library(cowplot)
library(magick)
library(dplyr)
#world_map <- map_data("newyork")


```


# Read Dataset 

```{r}
ds = read.csv("AB_NYC_2019.csv")
head(ds)
```

# Data cleaning 

## Check for NA and NULL values
```{r}

#Check for NA
apply(ds,2,function(x) sum(is.na(x)))

# NOTES
# Remove NA, empty
#
#
#
#

```

## Normalisation and selection of the variables

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}


clean_data = function(ds)
{
  ds = select (ds,-c(host_id, id, host_name, name,minimum_nights,number_of_reviews,
                     neighbourhood,last_review,availability_365,
                     
                     reviews_per_month,calculated_host_listings_count))
 

  numerical = c("price","longitude", "latitude")
  categorical = c("neighbourhood_group")
  
  ds[numerical] = scale(ds[numerical])
  ds$neighbourhood_group = factor(ds$neighbourhood_group, 
                                  level= c("Brooklyn","Manhattan",
                                           "Queens","Staten Island", "Bronx"), 
                                  labels=c(1,2,3,4,5))
  ds$room_type = factor(ds$room_type, 
                        level= c("Private room","Entire home/apt","Shared room"), 
                        labels=c(1,2,3))
  
  return(ds)
}
#ggdraw() +
#  draw_image("New_York_City_.png") +
#  draw_plot(myplot)

dataset = clean_data(ds)

head(dataset)
```

# K-MEANS 

**x**: numeric matrix, numeric data frame or a numeric vector
**centers**: Possible values are the number of clusters (k) or a set of initial (distinct) cluster centers. If a number, a random set of (distinct) rows in x is chosen as the initial centers.
**iter.max**: The maximum number of iterations allowed. Default value is 10.
**nstart**: The number of random starting partitions when centers is a number. Trying nstart > 1 is often recommended.


```{r}
km.res = kmeans(dataset, 4, nstart = 25)

cat("First 10 Clusters association",km.res$cluster[1:10])
cat("\nCenters")
print(km.res$centers)
cat("\ntotss",km.res$totss)
cat("\nwithinss",km.res$withinss)
cat("\ntot_withinss",km.res$tot.withinss)

cat("\nbetweenss",km.res$betweenss)
cat("\nSize",km.res$size)
cat("\niter",km.res$iter)
cat("\nifault",km.res$ifault)

```

To create a beautiful graph of the clusters generated with the kmeans() function, will use the factoextra package.
```{r}
library(factoextra)

```

Cluster number for each of the observations
```{r}
head(km.res$cluster)
```

Cluster size
```{r}
km.res$size
```

Cluster means
```{r}
km.res$centers
```

```{r}
#dataset$neighbourhood_group = as.numeric( dataset$neighbourhood_group)
#dataset$room_type = as.numeric(  dataset$room_type)
#fviz_cluster(km.res, data = dataset,
#             palette = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"),
#             ggtheme = theme_minimal(),
#             main = "Partitioning Clustering Plot"
#)

#res <- hcut(dataset, k = 4, stand = FALSE)
#fviz_dend(km.res, rect = TRUE, cex = 0.5,
#          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"))

```



# PAM ALGORITHM 

https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3
```{r}
library(cluster)
library(readr)
library(Rtsne)
```


Compute Gower distance
```{r}
dim(dataset)

smp_size <- floor(0.9 * nrow(dataset))
set.seed(123)

train_ind <- sample(seq_len(nrow(dataset)), size = smp_size)

prova = dataset[-train_ind,]
pam.res <- pam(prova, 4)

gower_dist <- daisy(prova, metric = "gower")

```

```{r}

start.time <- Sys.time()
sil_width <- c(NA)
for(i in 2:8){  
  pam_fit <- pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}


end.time <- Sys.time()
time.taken <- end.time - start.time

print("-- Time: -- ")
time.taken
print("")

plot(1:8, sil_width,
      xlab = "Number of clusters",
      ylab = "Silhouette Width")
lines(1:8, sil_width)

```

# FAMD

http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/115-famd-factor-analysis-of-mixed-data-in-r-essentials/
https://nextjournal.com/pc-methods/calculate-pc-mixed-data
https://cran.r-project.org/web/packages/FactoMineR/index.html
https://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont

```{r}

library("FactoMineR")
library("factoextra")
```

FAMD (base, ncp = 5, sup.var = NULL, ind.sup = NULL, graph = TRUE)
- base : a data frame with n rows (individuals) and p columns (variables).
- ncp: the number of dimensions kept in the results (by default 5)
- sup.var: a vector indicating the indexes of the supplementary variables.
- ind.sup: a vector indicating the indexes of the supplementary individuals.
- graph : a logical value. If TRUE a graph is displayed.

```{r}

res.famd <- FAMD(dataset, graph = FALSE, ncp = 10)
print(res.famd)
```


```{r}
eig.val <- get_eigenvalue(res.famd)
head(eig.val)

fviz_screeplot(res.famd)

```

```{r}
var <- get_famd_var(res.famd)
var
```

```{r}
# Coordinates of variables
head(var$coord)
# Cos2: quality of representation on the factore map
head(var$cos2)
# Contributions to the  dimensions
head(var$contrib)
```

```{r}
# Plot of variables
fviz_famd_var(res.famd, repel = TRUE)
# Contribution to the first dimension
fviz_contrib(res.famd, "var", axes = 1)
# Contribution to the second dimension
fviz_contrib(res.famd, "var", axes = 2)
# Contribution to the third dimension
fviz_contrib(res.famd, "var", axes = 3)
# Contribution to the forth dimension
fviz_contrib(res.famd, "var", axes = 4)
# Contribution to the fifth dimension
fviz_contrib(res.famd, "var", axes = 5)
# Contribution to the sixth dimension
fviz_contrib(res.famd, "var", axes = 6)
# Contribution to the seventh dimension
fviz_contrib(res.famd, "var", axes = 7)
# Contribution to the eighth dimension
fviz_contrib(res.famd, "var", axes = 8)
```

# PCAmixdata

```{r}

## Import library
library(PCAmixdata)

```

```{r}

## Split mixed dataset into quantitative and qualitative variables
## For now excluding the target variable "Churn", which will be added later as a supplementary variable
split <- splitmix(dataset[1:5])  

## PCA
res.pcamix <- PCAmix(X.quanti=split$X.quanti,  
                     X.quali=split$X.quali, 
                     rename.level=TRUE, 
                     graph=FALSE, 
                     ndim=25)

res.pcamix


```

```{r}

## Inspect principal components
res.pcamix$eig
```

```{r}

# Use Scree Diagram to select the components:
plot(res.pcamix$eig, type="b", main="Scree Diagram", xlab="Number of Component", ylab="Eigenvalues")
abline(h=1, lwd=3, col="red")
```


# Hierarchical Cluster Analysis



```{r}
library(cluster)    # clustering algorithms
library(dendextend) # for comparing two dendrograms

```


```{r}
reduced <- dataset[ sample(1:nrow(dataset), nrow(dataset)/10 ) , ]
d <- dist(reduced, method = "euclidean")


# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(reduced, method = x)$ac
}

library(purrr)
map_dbl(m, ac)

```

```{r}
hc3 <- agnes(reduced, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 

```

# Clust miX type
k-prototypes in RAn implementation of the k-prototypes algorithm is given by the function

kproto(x, k, lambda = NULL, iter.max = 100, nstart = 1, na.rm = TRUE)

where
•x is a data frame with both numeric and factor variables.  As opposed to other existing Rpackages, the factor variables do not need to be preprocessed in advance and the order of thevariables does not matter.
•k is the number of clusters which has to be pre-specified. Alternatively, it can also be a vectorof observation indices or a data frame of prototypes with the same columns asx.  If ever atthe initialization or during the iteration process identical prototypes do occur, the number ofclusters will be reduced accordingly.

•lambda>0 is a real valued parameter that controls the trade off between Euclidean distancefor numeric variables and simple matching distance for factor variables for cluster assignment.If noλis specified the parameter is set automatically based on the data and a heuristic usingthe functionlambdaest(). Alternatively, a vector of lengthncol(x)can be passed tolambda(cf.Section on Extensions to the original algorithm).

•iter.maxsets the maximum number of iterations, just as inkmeans(). The algorithm may stopprior toiter.maxif no observations swap clusters.

•nstartmay be set to a value>1 to run k-prototypes multiple times. Similar to k-means, theresult of k-prototypes depends on its initialization. Ifnstart>1, the best solution (i.e. the onethat minimizesE) is returned.

•Generally, the algorithm can deal with missing data but as a defaultNAs are removed byna.rm= TRUE

```{r}
library(clustMixType)

kp = kproto(dataset,5)
kp 

summary(kp)
```

```{r}

library(wesanderson)
par(mfrow=c(2,2))
clprofiles(kp, dataset, col = wes_palette("Royal1",5, type = "continuous"))  
```

```{r}
Es = numeric(10)
for(i in 1:10)
  {
    kpres <- kproto(dataset, k = i, nstart = 5)
    Es[i] <- kpres$tot.withinss
  }
plot(1:10, Es, type = "b", ylab = "Objective Function", xlab = "# Clusters",main = "Scree Plot") 
```